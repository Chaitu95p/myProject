Mapreduce Advanced Concepts (05-27 NS)


•	Coding Efficiency			In Developers Hand
•	Configuration Properties		In Admin Hands
•	Cluster Hardware		 	In Infrastructure Team Hands

 	No.of Containers = No.of tasks we can able to run in parallel.

Eg: 48 Containers means, we’ve a capacity of running 48 tasks in parallel.

256 GB Ram in Node	=	32 GB for Linux OS + 32 GB for HBase + 64 GB for Impala + Rest of the RAM i.e,. 128 GB ram for YARN.

24 Cores 	 48 Vcores	48 Containers

•	In general all nodes have same configurations in the cluster.

YARN Job Anatomy:
----------------

Client
Resource Manager
Node Manager
MapReduce Application Master
HDFS

* Client submits a MR job(i.e., .jar file)
* Job will get submitted to ResourceManager(RM runs on 8032 IPC Port)
* Get the Job ID (job_epoch timestamp from RM started_seq number) eg. (job_1462348973456_0001)
* Once it gets Job ID, It'll Copies Job Resources(.JAR file and dependent .jar files are copied from LFS to HDFS /tmp dir) to HDFS
* ResourceManager identifies one NodeManager on which some containers will available and inside container tasks will run.
* NodeManager launches MRAppMaster.
* Retrieve InputSplits from HDFS.
* AppMaster requests resources to ResourceManager.

NOTE: For one job one AppMaster will be there.
      If you have 100 jobs parallely running on your cluster then 100 AppMasters will be there.


*** Speculative Execution.

org.apache.hadoop.mapred (Old MapReduce API)

org.apache.hadoop.mapreduce
org.apache.hadoop.mapreduce.lib  (New MapReduce API i.e., MRv2)

* Don't mix new API code with Old API code.


To Unit Test your MapReduce programs we've MRUnit which is built by extending the JUnit Framework.
mrunit-1.0.0-hadoop2.jar

* Data transfer will be less when we use Combiner. -- Combiner is just like a pre reducer.
* Combiners can be treated as "Local Reducers" in Mapper phase.
