4 Pillars of Hadoop Security:-

  1) Authentication with Kerberos
  2) Authorization using Sentry, ACL
  3) HDFS encryption using Java KMS
  4) Auditing & Lineage (Using Cloudera Navigator)

** BDR ---> Cloudera Backup and Disaster Recovery
            Available only with a Cloudera Enterprise license.
            
Pre-requisites for Installing CDH cluster:  (Just config files mentioned. Watch 2nd video for detailed info)
  -- /etc/rc.local  (To disable transparent huge pages)
  -- /etc/sysctl.conf   (Change VM swappinnes)
  -- Disable firewall
  -- Disable SELinux
  -- /etc/sysconfig/network .............  Watch Video
  
  -- /etc/yum.repos.d 
  
** Need to know about password less authentication between cluster nodes.

Imp Commands:
-----------

$ yum repolist
$ vgdisplay
$ yum -y install <Required-command>     (Used to insatll any linux command)
$ yum list all
$ yum clean all   (To clear previous cache)
$ yum makecache   (To update latest cache file)
$ authconfig-tui

$ mysql -u <username> -p

Centralized Authentication used in hadoop servers (cluster):-
-----------------------------------------------------------

-- Hadoop cluster is nothing but a group of servers/machines/nodes. So each server is having an OS(It can be RHEL/CentOS/windows etc.,).
   So each OS is having a different authentication mechanism. We can create user in each server in the cluster and we can work. 
   Here the main problem will arise when we implement Kerberos authentication. In Kerberos authentication the same user should be 
   present in all the servers. If it's a 1000 nodes cluster then it'll be a problem to create a same user in all 1000 nodes manually. 
-- To overcome this problem we can use either Active Directory (LDAP + Kerberos) or OpenLDAP.
   LDAP is a protocal. 
   
   * AD or OpenLDAP implemented server will be integrated with all the servers in the cluster. 
     When a new user wants access to hadoop cluster, Simply admins will create a user in AD or OpenLDAP implemented server. 
     With that created credentials that user can login into hadoop cluster based on assigned groups. 
     Here no need to create user in each and every server in the cluster.
   
   * If AD is used, then to ingrate with unix/linux boxes in the cluster we need 3rd party softwares 
     like Centrify, Quest authentication, SSSD etc.,
     
   * php LDAP admin is a GUI tool to create users and groups.
   
   
***********  LDAP URL = ldaps://ctlldapodc.ctl.intranet:636 (Check in HUE configuration in CM)

Adding new host to CDH cluster using Cloudera Manager


Cloudera Kerberization:-
----------------------

$ yum install krb5-server krb5-libs krb5-workstation

edit /etc/krb5.conf file with kdc server name name 

$ kdb5_util create -s

** Installion & configuration of MIT kerberos KDC server and Kerberizing CDH cluster using cloudera manager

- Select any one server in the cluster as MIT kerberos server.
https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.3.0/enabling-kerberos/content/(optional)_install_a_new_mit_kdc.html

Note: We know how to install & configure MIT kerberos server and how to integrate that kerberos with our cluster using cloudera manager.
----  Once it's done how to access cluster.

Kerberos principal and keytab creation

ticket-granting ticket (tgt)

** Anyone who need root privilege need to copy root.keytab file and place it somewhere securely and find out 
   the principal name of root user.

$ kinit -kt <keytab file> <principal name>

Q) Which tgt you own?
   $ klist
Q) To destroy/clean tgt?
   $ kdestroy


Sentry:-
------

Sentry is just for authorization ( setting up proper permission for users)
-- We can control hive and impala databases, tables, coloums etc.,
-- We can define who can access hive/impala database objects.

Kerberos is one of the pre-requisite for Sentry.

Configuration of Sentry for Hive authorization:-

[ab90629@polpcdhen001 ~]$ id ab90629
uid=77313300(ab90629) gid=77313300(ab90629) groups=77313300(ab90629),543835435(cdh_prod_cm_readonly),
543835428(cdh_prod_cn_lineageviewer),543843913(cdh_prod_pentaho_users),543872767(cdlappgrpp),
543795510(hivebdalabupdatep),543751128(hivereadonlyallp),543891441(jupyteruserp),543835477(sucdlappp)
[ab90629@polpcdhen001 ~]$ id cdlapp
uid=38882300(cdlapp) gid=38882300(cdlapp) groups=38882300(cdlapp),543835435(cdh_prod_cm_readonly),
543872767(cdlappgrpp),543872765(cdlsftpgrp),543795510(hivebdalabupdatep)
[ab90629@polpcdhen001 ~]$

[cdlapp@polpcdhen001 ~]$ alias kti
alias kti='kinit -kt ~/cdlapp.keytab cdlapp@CTL.INTRANET'

* Without Sentry - This is kind of WIDE OPEN ACCESS i.e., anyone can access anyones data(database/tables etc.,)
  So it's not the case in Enterprises. So we need to use sentry for authorization.
  
* We can do Sentry administration either from the hive or impala or hue.
	eg: > CREATE ROLE admin_role;
		> SHOW ROLES;
		> GRANT ALL ON SERVER server1 TO ROLE admin_role;
		> GRANT ROLE admin_role TO sentryadmin
		(So whoever belongs to sentryadmin group will have admin_role privilege. It has entire server access.)

Cloudera Hadoop Log Managment:-

1) Logs related to Cloudera Manager services,
2) Logs related to Ecosystem components &
3) YARN Logs(Very important for trobleshooting a failed job).


1---> 	Log file exists on Cloudera Manager Server Node.
		Log dir === /var/log/cloudera-scm-server/
		Log file == cloudera-scm-server.log
		
		Log file for Cloudera Manager Agents will exists on all Agent Nodes(i.e., all nodes in the cluster)
		/var/log/cloudera-scm-agent/
		cloudera-scm-agent.log
2----> Check in Cloudera Manager service specific configuration for knowing log directory path.
3----> For trobleshooting YARN jobs you need to go into ResourceManager WebUI.


/etc/cloudera-scm-agent/config.ini


Dynamic Resource pool configuration in Cloudera:-

** Dynamic resource pools are named configuration of resources and policy for scheduling this resources among YARN applications.

Tuning YARN:-

https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_yarn_tuning.html

Tuning is nothing but defining or changing certain parameters at YARN level.
** In YARN, We've seen the term Called Container(CPU (Vcores) + Memory).

Cloudera Manager Server Backup:-

We need to take the backup of back end databases(default db is PostgreSQL) and some directories like /etc/cloudera-scm-server 
and /var/lib/cloudera-scm-server directories. We can use these backup in case of failure and migrate 
cloudera manager server service to a new node.

Connecting to PostgreSQL db

> psql -U <username> -p 7432 -d <db name>

** To take backup of any file use "pg_dump" command.
** To take backup of entire db use "pg_dumpall" command.

Restoring Cloudera Manager From Backup / Migrating Cloudera Manager Server to New Node:-

Migrating Cloudera Manager server to a new node.We can do this in case of Cloudera Manager Server failure or as part of 
planned migration activity.

Migrating namenode service from one server to another. In case of maintenance or Namenode failure we can use this approach 
to recover the namenode

** The name node directory contains fsimage and edit files inside .

HDFS snapshots:-

HDFS Snapshots are read-only point-in-time copies of the file system.
Snapshots can be taken on a subtree of the file system or the entire file system.
Some common use cases of snapshots are data backup, protection against user errors and disaster recovery.
Nested snapshottable directories are currently not allowed. 

Enabling snapshot
hdfs dfsadmin -allowSnapshot <path>
Diabling snapshot
hdfs dfsadmin -disallowSnapshot <path>

** If snapshot is not enabled then we'll get below error

	[ab90629@poldcdhen001 ~]$ hdfs dfsadmin -allowSnapshot WCmrDir
	allowSnapshot: Access denied for user ab90629. Superuser privilege is required
	
	[ab90629@poldcdhen001 ~]$ hdfs dfs -createSnapshot WCmrDir
	createSnapshot: Directory is not a snapshottable directory: /user/ab90629/WCmrDir
	
https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html

HDFS Snapshot management using Cloudera Manager:-

HDFS Backup and Restore process using HDFS snapshot. Here we're using Cloudera Manager to manage snapshots.
We need Cloudera Enterprise or Enterprise Trail license to test this feature.

HDFS service in CM --> File Browser
